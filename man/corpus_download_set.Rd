% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/corpus_download_set.r
\name{corpus_download_set}
\alias{corpus_download_set}
\title{Download Corpus}
\usage{
corpus_download_set(
  pages_dir = file.path(".", "data", "pages"),
  title_and_abstract_search,
  continue = TRUE,
  delete_pages_dir = FALSE,
  set_size = 1000,
  verbose = TRUE,
  dry_run = FALSE,
  mc_cores = 3
)
}
\arguments{
\item{pages_dir}{The directory where the downloaded pages will be stored. Default is "./data/pages".}

\item{title_and_abstract_search}{The search query for the title and abstract of the documents to be downloaded.}

\item{continue}{Logical indicating whether to continue downloading from where it left off. Default is TRUE.}

\item{delete_pages_dir}{Logical indicating whether to delete the pages directory before downloading.
Default is FALSE.}

\item{set_size}{The number of works to be downloaded in each set. Default is 1000.}

\item{verbose}{Logical indicating whether to display progress messages. Default is TRUE.}

\item{dry_run}{Logical indicating whether to run the function without downloading any data. Default is FALSE.}

\item{mc_cores}{The number of cores to be used for parallel processing. Default is 3.
This is limiting the number of parallel downloads}
}
\value{
None
}
\description{
This function downloads a corpus of documents based on a given title and abstract search query.
}
\examples{
\dontrun{
download_corpus(title_and_abstract_search = "climate change")
}
}
